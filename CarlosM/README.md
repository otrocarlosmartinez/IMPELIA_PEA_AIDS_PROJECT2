# PROYECTO 2
**An√°lisis Avanzado y Modelado Predictivo de Precios de Viviendas en Barcelona utilizando KNIME, AutoML y Power BI**

### Objective
Expand the analysis and predictive modeling of housing prices in Barcelona using advanced tools such as KNIME for ETL and analysis, Power BI for interactive visualization, and AutoML tools as a Low-Code or No-Code Machine Learning platform. The goal is to **improve the accuracy of the predictive model and provide interactive visualizations** that facilitate decision making.

### Project Approach
Since this is a continuation of Project 1, with a database that maintains its structure (same variables) and expands the number of records, the following stages are proposed:

1. Python. Data analysis and modeling using Python, adapting Project 1 to the new expanded database. This stage will serve as a basis for comparisons of both the results and the process carried out with the proposed tools. 

2. KNIME / PowerBI. Data analysis and modeling using KNIME and PowerBI to be evaluated on a separated files

3. AutoML. Data analysis and modeling using Machine Learning and AI tools in the capacities provided by the free versions of the same. The following tools are initially considered:
	- Google AutoML
	- BigML
	- H2o.ai
	- Microsoft Azure Machine Learning
	- Obviously.ai
	- IBM's Watson
	- Amazon's Sage Maker
	- streamlit.io

### Project Deliverables
1. Several working files will be created each with its own conclusions about the problem at hand.
2. Consolidated report focusing on process and results comparison considering DSLC framework.

### Reference
#### Data Science Life Cycle (DSLC)
A Data Science project that follows the Data Science Life Cycle (DSLC) adheres to a structured and iterative process for solving data-related problems, ensuring that the solution is robust, scalable, and aligned with business objectives. The DSLC typically consists of the following stages:

1. **Problem Definition**: Clearly defining the business problem or question to be solved. This ensures the project's objectives are aligned with organizational goals.
2. **Data Collection**: Gathering relevant data from various sources, such as databases, APIs, or external datasets, ensuring it supports the problem statement.
3. **Data Preparation**: Cleaning, preprocessing, and organizing the data. This includes handling missing values, outliers, data transformations, and feature engineering.
4. **Exploratory Data Analysis (EDA)**: Analyzing the data to understand patterns, relationships, and potential anomalies. This step often involves data visualization and statistical analysis to generate insights.
5. **Modeling**: Selecting and applying appropriate machine learning or statistical models. This step includes training, validating, and fine-tuning models to optimize their performance.
6. **Evaluation**: Assessing the model's performance using metrics such as accuracy, precision, recall, or others relevant to the project. Ensuring the model meets the required standards for deployment.
7. **Deployment**: Implementing the model in a production environment, making it accessible for real-world use. This might involve integrating the model with existing systems or deploying it via APIs or cloud platforms.
8. **Monitoring and Maintenance**: Continuously monitoring the model's performance in production to ensure its accuracy and relevance over time. This stage may also involve retraining the model as new data becomes available.
9. **Communication and Reporting**: Presenting findings and results to stakeholders in a clear and actionable manner, often through dashboards, visualizations, or reports.


	



