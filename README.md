# Repository for an academic Data Science project.

- **Academy**: IMPELIA
- **Program**: PROGRAMA DE ESPECIALIZACIÓN AVANZADA EN INTELIGENCIA ARTIFICIAL Y CIENCIA DE DATOS 
- **Project**: Análisis Avanzado y Modelado Predictivo de Precios de Viviendas en Barcelona utilizando KNIME, AutoML y Power BI
- **Team**: Carlos Martinez Vasquez; ​Imalvet Santiesteban Garcia; ​Guillem Edo Bertran

### Project Deliverables
1. Several working files will be created each with its own conclusions about the problem at hand.
2. Project delivery files organized on folders by tool (PYTHON, POWERBI, KNIME, AUTOML)
3. README file per each tool's folder

### Repository Structure:
- Contributor folders: one folder per team member for individual work
- PROYECTO II: Containing the team's consolidated final delivery
- **PYTHON**
    - README_PYTHON.md -> Project notes
    - PROJECT2_PYTHON.ipynb -> Main project file
    - PROJECT2_PYTHON.html -> Main project file in html format for web visualization
    - model_retrain.py -> Script for model retraining
    - PROJECT2UI.py -> User interface code
    - models/ -> Containing the model files
    - .csv files -> Containing the original and modified data files
    - .png files -> Containing reference images
- **POWERBI**
    - README_POWERBI.md -> Project notes
    - PROJECT2_POWERBI.pbix -> Main project file
    - data_files/ -> Containing the data files from PYTHON
    - .png files -> Containing reference images



### Reference
#### Data Science Life Cycle (DSLC)
A Data Science project that follows the Data Science Life Cycle (DSLC) adheres to a structured and iterative process for solving data-related problems, ensuring that the solution is robust, scalable, and aligned with business objectives. The DSLC typically consists of the following stages:

1. **Problem Definition**: Clearly defining the business problem or question to be solved. This ensures the project's objectives are aligned with organizational goals.
2. **Data Collection**: Gathering relevant data from various sources, such as databases, APIs, or external datasets, ensuring it supports the problem statement.
3. **Data Preparation**: Cleaning, preprocessing, and organizing the data. This includes handling missing values, outliers, data transformations, and feature engineering.
4. **Exploratory Data Analysis (EDA)**: Analyzing the data to understand patterns, relationships, and potential anomalies. This step often involves data visualization and statistical analysis to generate insights.
5. **Modeling**: Selecting and applying appropriate machine learning or statistical models. This step includes training, validating, and fine-tuning models to optimize their performance.
6. **Evaluation**: Assessing the model's performance using metrics such as accuracy, precision, recall, or others relevant to the project. Ensuring the model meets the required standards for deployment.
7. **Deployment**: Implementing the model in a production environment, making it accessible for real-world use. This might involve integrating the model with existing systems or deploying it via APIs or cloud platforms.
8. **Monitoring and Maintenance**: Continuously monitoring the model's performance in production to ensure its accuracy and relevance over time. This stage may also involve retraining the model as new data becomes available.
9. **Communication and Reporting**: Presenting findings and results to stakeholders in a clear and actionable manner, often through dashboards, visualizations, or reports.